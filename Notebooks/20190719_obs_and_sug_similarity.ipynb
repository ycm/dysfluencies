{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given categories for \\\\$obs and \\\\$sugs, relative similarity of evaluators' responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "ct = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from python script\n",
    "def script(obs):\n",
    "#     f = open('../Resources/20190713ReadingCategories.json')\n",
    "#     obs = json.load(f)\n",
    "#     f.close()\n",
    "\n",
    "    f = open('../Resources/categories_to_keywords.json')\n",
    "    categories = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    f = open('../Resources/categories_ignore.json')\n",
    "    ignore = {ps.stem(w) for w in json.load(f)}\n",
    "    f.close()\n",
    "    \n",
    "    obs = [\n",
    "        o.strip()\n",
    "        for line in obs\n",
    "        for o in line.split(';')\n",
    "    ]\n",
    "    \n",
    "    stemmed_words_to_categories = {}\n",
    "    for k, words in categories.items():\n",
    "        words = [ps.stem(w) for w in words]\n",
    "        for w in words:\n",
    "            stemmed_words_to_categories[w] = k\n",
    "\n",
    "    punct = '.,;:\\'\\\"-'\n",
    "\n",
    "    for i in range(len(obs)):\n",
    "        obs[i] = obs[i].lower()\n",
    "        obs[i] = obs[i].replace('word by word', 'wordbyword')\n",
    "        obs[i] = obs[i].replace('word-by-word', 'wordbyword')\n",
    "        obs[i] = obs[i].replace('high frequency', 'highfrequency')\n",
    "        obs[i] = obs[i].replace('self correct', 'selfcorrect')\n",
    "        obs[i] = obs[i].replace('self-correct', 'selfcorrect')\n",
    "        tokens = nltk.word_tokenize(obs[i])\n",
    "        tokens = [ps.stem(x) for x in tokens if x not in punct]\n",
    "        obs[i] = ' '.join(tokens)\n",
    "\n",
    "    uncategorized = []\n",
    "    for o in obs:\n",
    "        found = False\n",
    "        for w in o.split():\n",
    "            if w in stemmed_words_to_categories:\n",
    "                ct[stemmed_words_to_categories[w]] += 1\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            continue\n",
    "        uncategorized.append(o)\n",
    "\n",
    "    for cat, cnt in ct.most_common():\n",
    "        print(cat, cnt)\n",
    "    print('Caught:', sum([x[1] for x in ct.most_common()]))\n",
    "\n",
    "    print('\\n')\n",
    "    print('Uncategorized:', len(uncategorized))\n",
    "\n",
    "    not_caught = []\n",
    "    for line in uncategorized:\n",
    "        tokens = set(line.split())\n",
    "        if tokens & ignore == set():\n",
    "            not_caught.append(line)\n",
    "\n",
    "    print('Not caught:', len(not_caught))\n",
    "\n",
    "    for nc in not_caught:\n",
    "        # print(nc)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_to_obs = json.load(open('../WorkingJsons/eval_to_obs.json'))\n",
    "eval_to_sug = json.load(open('../WorkingJsons/eval_to_sug.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lindsay wyman\n",
      "omission_insertion 16\n",
      "fluency 11\n",
      "self_correction 8\n",
      "rate 6\n",
      "substitution_reversal 6\n",
      "phonics 5\n",
      "punctuation 5\n",
      "word_by_word 4\n",
      "expression 3\n",
      "morphology 3\n",
      "multisyllabic_words 3\n",
      "word_endings 2\n",
      "word_attack 2\n",
      "monitoring_for_meaning 2\n",
      "vocabulary 2\n",
      "phrasing 1\n",
      "accuracy 1\n",
      "Caught: 80\n",
      "\n",
      "\n",
      "Uncategorized: 56\n",
      "Not caught: 44\n",
      "\n",
      "\n",
      "sharilyn fetterhoff-bacci\n",
      "monitoring_for_meaning 31\n",
      "rate 29\n",
      "punctuation 29\n",
      "expression 27\n",
      "accuracy 26\n",
      "self_correction 25\n",
      "word_by_word 22\n",
      "fluency 16\n",
      "omission_insertion 16\n",
      "multisyllabic_words 14\n",
      "phrasing 14\n",
      "phonics 10\n",
      "sight_words 10\n",
      "substitution_reversal 9\n",
      "word_endings 9\n",
      "vocabulary 8\n",
      "word_attack 4\n",
      "pronunciation 4\n",
      "morphology 3\n",
      "Caught: 306\n",
      "\n",
      "\n",
      "Uncategorized: 118\n",
      "Not caught: 106\n",
      "\n",
      "\n",
      "susan barber\n",
      "phrasing 71\n",
      "monitoring_for_meaning 59\n",
      "expression 57\n",
      "punctuation 56\n",
      "self_correction 53\n",
      "rate 48\n",
      "accuracy 47\n",
      "word_by_word 31\n",
      "omission_insertion 31\n",
      "substitution_reversal 21\n",
      "fluency 19\n",
      "vocabulary 15\n",
      "sight_words 15\n",
      "multisyllabic_words 14\n",
      "phonics 12\n",
      "word_endings 10\n",
      "word_attack 8\n",
      "pronunciation 5\n",
      "morphology 3\n",
      "Caught: 575\n",
      "\n",
      "\n",
      "Uncategorized: 122\n",
      "Not caught: 105\n",
      "\n",
      "\n",
      "talia kovacs\n",
      "phrasing 101\n",
      "expression 63\n",
      "monitoring_for_meaning 59\n",
      "punctuation 58\n",
      "rate 57\n",
      "self_correction 57\n",
      "accuracy 47\n",
      "omission_insertion 35\n",
      "substitution_reversal 32\n",
      "word_by_word 31\n",
      "fluency 26\n",
      "word_endings 16\n",
      "vocabulary 16\n",
      "sight_words 15\n",
      "multisyllabic_words 14\n",
      "phonics 13\n",
      "word_attack 8\n",
      "pronunciation 6\n",
      "morphology 3\n",
      "Caught: 657\n",
      "\n",
      "\n",
      "Uncategorized: 46\n",
      "Not caught: 45\n",
      "\n",
      "\n",
      "carolyn greenberg\n",
      "phrasing 103\n",
      "punctuation 78\n",
      "expression 76\n",
      "self_correction 73\n",
      "monitoring_for_meaning 69\n",
      "rate 59\n",
      "accuracy 53\n",
      "omission_insertion 35\n",
      "substitution_reversal 32\n",
      "word_by_word 31\n",
      "sight_words 29\n",
      "fluency 26\n",
      "word_endings 23\n",
      "phonics 18\n",
      "vocabulary 16\n",
      "multisyllabic_words 14\n",
      "word_attack 8\n",
      "pronunciation 7\n",
      "morphology 3\n",
      "Caught: 753\n",
      "\n",
      "\n",
      "Uncategorized: 36\n",
      "Not caught: 31\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e, lst in eval_to_obs.items():\n",
    "    print(e)\n",
    "    script(lst)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sug_script(lst):\n",
    "    sugs = [\n",
    "        o.strip()\n",
    "        for line in lst\n",
    "        for o in line.split(';')\n",
    "    ]\n",
    "    ct = Counter()\n",
    "    punct = '.,;:\\'\\\"-()'\n",
    "\n",
    "#     f = open('../WorkingJsons/all_sugs.json')\n",
    "#     sugs = json.load(f)\n",
    "#     f.close()\n",
    "\n",
    "    for i in range(len(sugs)):\n",
    "        sugs[i] = ' '.join([ps.stem(w) for w in nltk.word_tokenize(sugs[i])])\n",
    "\n",
    "    f = open('../Resources/categories_to_keywords_sugs.json')\n",
    "    categories = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    stemmed_words_to_categories = {}\n",
    "    for k, words in categories.items():\n",
    "        words = [ps.stem(w) for w in words]\n",
    "        for w in words:\n",
    "            stemmed_words_to_categories[w] = k\n",
    "    # print(stemmed_words_to_categories)\n",
    "\n",
    "    caught = 0\n",
    "    uncategorized = []\n",
    "    for i in range(len(sugs)):\n",
    "        sugs[i] = sugs[i].replace('self-correct', 'selfcorrect')\n",
    "        sugs[i] = sugs[i].replace('high frequency', 'highfrequency')\n",
    "        sugs[i] = sugs[i].replace('self-monitor', 'selfmonitor')\n",
    "        sugs[i] = sugs[i].replace('more difficult', 'harder')\n",
    "        found = False\n",
    "        tokens = [x for x in nltk.word_tokenize(sugs[i]) if x not in punct]\n",
    "        for tkn in tokens:\n",
    "            if tkn in stemmed_words_to_categories:\n",
    "                ct[stemmed_words_to_categories[tkn]] += 1\n",
    "                found = True\n",
    "                caught += 1\n",
    "                break\n",
    "        if not found:\n",
    "            uncategorized.append(sugs[i])\n",
    "\n",
    "    for i, cnt in ct.most_common():\n",
    "        print(i, cnt)\n",
    "    print('\\n')\n",
    "\n",
    "    print('Caught:', caught)\n",
    "    print('Uncategorized:', len(uncategorized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carolyn greenberg\n",
      "meaning_comprenhension 39\n",
      "fluency 22\n",
      "phonics 11\n",
      "difficulty 4\n",
      "expression 3\n",
      "sight_word 2\n",
      "word_endings 1\n",
      "\n",
      "\n",
      "Caught: 82\n",
      "Uncategorized: 23\n",
      "\n",
      "\n",
      "sharilyn fetterhoff-bacci\n",
      "expression 33\n",
      "meaning_comprenhension 21\n",
      "self_monitor 21\n",
      "fluency 21\n",
      "multisyllabic_words 15\n",
      "punctuation 15\n",
      "sight_word 13\n",
      "phrasing 12\n",
      "word_endings 11\n",
      "slower_pace 9\n",
      "vocabulary 9\n",
      "self_correction 8\n",
      "difficulty 6\n",
      "pronunciation 4\n",
      "phonics 4\n",
      "pausing 3\n",
      "\n",
      "\n",
      "Caught: 205\n",
      "Uncategorized: 128\n",
      "\n",
      "\n",
      "susan barber\n",
      "difficulty 62\n",
      "phrasing 44\n",
      "self_monitor 32\n",
      "word_attack 32\n",
      "sight_word 30\n",
      "slower_pace 16\n",
      "expression 12\n",
      "word_endings 8\n",
      "phonics 7\n",
      "self_correction 3\n",
      "pronunciation 3\n",
      "fluency 2\n",
      "punctuation 2\n",
      "vocabulary 2\n",
      "pausing 1\n",
      "meaning_comprenhension 1\n",
      "\n",
      "\n",
      "Caught: 257\n",
      "Uncategorized: 51\n",
      "\n",
      "\n",
      "lindsay wyman\n",
      "fluency 26\n",
      "difficulty 12\n",
      "phonics 10\n",
      "word_endings 8\n",
      "self_correction 7\n",
      "meaning_comprenhension 6\n",
      "multisyllabic_words 5\n",
      "punctuation 4\n",
      "word_attack 2\n",
      "slower_pace 2\n",
      "phrasing 2\n",
      "self_monitor 1\n",
      "sight_word 1\n",
      "vocabulary 1\n",
      "\n",
      "\n",
      "Caught: 87\n",
      "Uncategorized: 47\n",
      "\n",
      "\n",
      "talia kovacs\n",
      "difficulty 21\n",
      "pausing 12\n",
      "fluency 10\n",
      "slower_pace 6\n",
      "meaning_comprenhension 6\n",
      "punctuation 5\n",
      "expression 4\n",
      "phonics 4\n",
      "word_endings 3\n",
      "vocabulary 2\n",
      "self_monitor 1\n",
      "pronunciation 1\n",
      "\n",
      "\n",
      "Caught: 75\n",
      "Uncategorized: 43\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e, lst in eval_to_sug.items():\n",
    "    print(e)\n",
    "    sug_script(lst)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
