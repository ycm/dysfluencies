{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model results after introducing token-based difference scores.\n",
    "\n",
    "- No regard for token frames.\n",
    "\n",
    "- Six difference scores constructed by averaging sequence differences across gold examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.append('util')\n",
    "import util\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_categories, sug_categories = util.load_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ACCURACY',\n",
       "  'EXPRESSION',\n",
       "  'FLUENCY',\n",
       "  'MONITORING_FOR_MEANING',\n",
       "  'MORPHOLOGY',\n",
       "  'MULTISYLLABIC_WORDS',\n",
       "  'OMISSION_INSERTION',\n",
       "  'PHONICS',\n",
       "  'PHRASING',\n",
       "  'PRONUNCIATION',\n",
       "  'PUNCTUATION',\n",
       "  'RATE',\n",
       "  'SELF_CORRECTION',\n",
       "  'SIGHT_WORD',\n",
       "  'SUBSTITUTION_REVERSAL',\n",
       "  'VOCABULARY',\n",
       "  'WORD_ATTACK',\n",
       "  'WORD_BY_WORD',\n",
       "  'WORD_ENDINGS'],\n",
       " ['ARTICULATION',\n",
       "  'DIFFICULTY',\n",
       "  'EXPRESSION',\n",
       "  'FLUENCY',\n",
       "  'MEANING_COMPRENHENSION',\n",
       "  'MORPHOLOGY',\n",
       "  'MULTISYLLABIC_WORDS',\n",
       "  'OMISSIONS_INSERTIONS',\n",
       "  'PHONICS',\n",
       "  'PHRASING',\n",
       "  'PRONUNCIATION',\n",
       "  'PUNCTUATION',\n",
       "  'RATE',\n",
       "  'SELF_CORRECTION',\n",
       "  'SELF_MONITOR',\n",
       "  'SIGHT_WORD',\n",
       "  'SUBSTITUTIONS_REVERSALS',\n",
       "  'VOCABULARY',\n",
       "  'VOICE',\n",
       "  'WORD_ATTACK',\n",
       "  'WORD_ENDINGS'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_categories, sug_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare matrices again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('res/reading_examples_with_token_features.json') as f:\n",
    "#     reading_examples = json.load(f)\n",
    "with open('res/reading_examples_with_token_features_normalized.json') as f:\n",
    "    reading_examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_types = [\n",
    "    'StrippedPosDiff',\n",
    "    'StrippedNegDiff',\n",
    "    'StrippedMinorDiff',\n",
    "    'NoPausePosDiff',\n",
    "    'NoPauseNegDiff',\n",
    "    'NoPauseMinorDiff'\n",
    "]\n",
    "def fetch_diff_scores(ex):\n",
    "    return [ex[t] for t in diff_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_header = '\\t'.join(obs_categories + diff_types)\n",
    "labels_header = '\\t'.join(sug_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_as_example(obs_scores, sug_scores, diff_scores):\n",
    "    ret_obs = [obs_scores[c] for c in obs_categories]\n",
    "    ret_sug = [sug_scores[c] for c in sug_categories]\n",
    "    return '\\t'.join([str(x) for x in ret_obs + diff_scores]), '\\t'.join([str(x) for x in ret_sug])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try featurizing by evaluation, later try by example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ternerize(d):\n",
    "    for k, v in d.items():\n",
    "        if v > 1:\n",
    "            d[k] = 1\n",
    "        if v < -1:\n",
    "            d[k] = -1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "for ex in reading_examples:\n",
    "    diff_scores = fetch_diff_scores(ex)\n",
    "    for ev in ex['Evaluations']:\n",
    "        obs_scores = {c: 0 for c in obs_categories}\n",
    "        sug_scores = {c: 0 for c in sug_categories}\n",
    "        for statement, categories in ev['Observations'].items():\n",
    "            if categories[0] == 'NEUTRAL':\n",
    "                continue\n",
    "            polarity = 1 if categories[0] == 'POSITIVE' else -1\n",
    "            for category in categories[1:]:\n",
    "                obs_scores[category] += polarity\n",
    "        for statement, categories in ev['Suggestions'].items():\n",
    "            for category in categories:\n",
    "                sug_scores[category] += 1\n",
    "        ts_features, ts_labels = combine_as_example(obs_scores, ternerize(sug_scores), diff_scores)\n",
    "        features.append(ts_features)\n",
    "        labels.append(ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222, 222)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('res/features_by_ex_with_normalized_token_scores.tsv', 'w') as f:\n",
    "#     print(features_header, file=f)\n",
    "#     for feature in features:\n",
    "#         print(feature, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('res/labels_by_ex_with_normalized_token_scores.tsv', 'w') as f:\n",
    "#     print(labels_header, file=f)\n",
    "#     for label in labels:\n",
    "#         print(label, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('res/features_by_eval_with_token_scores.tsv', 'w') as f:\n",
    "#     print(features_header, file=f)\n",
    "#     for feature in features:\n",
    "#         print(feature, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('res/labels_by_eval_with_token_scores.tsv', 'w') as f:\n",
    "#     print(labels_header, file=f)\n",
    "#     for label in labels:\n",
    "#         print(label, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "for ex in reading_examples:\n",
    "    diff_scores = fetch_diff_scores(ex)\n",
    "    obs_scores = {c: 0 for c in obs_categories}\n",
    "    sug_scores = {c: 0 for c in sug_categories}\n",
    "    for ev in ex['Evaluations']:\n",
    "        for statement, categories in ev['Observations'].items():\n",
    "            if categories[0] == 'NEUTRAL':\n",
    "                continue\n",
    "            polarity = 1 if categories[0] == 'POSITIVE' else -1\n",
    "            for category in categories[1:]:\n",
    "                obs_scores[category] += polarity\n",
    "        for statement, categories in ev['Suggestions'].items():\n",
    "            for category in categories:\n",
    "                sug_scores[category] += 1\n",
    "    ts_features, ts_labels = combine_as_example(ternerize(obs_scores), ternerize(sug_scores), diff_scores)\n",
    "    features.append(ts_features)\n",
    "    labels.append(ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('res/features_by_ex_with_normalized_token_scores.tsv', 'x') as f:\n",
    "#     print(features_header, file=f)\n",
    "#     for feature in features:\n",
    "#         print(feature, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('res/labels_by_ex_with_normalized_token_scores.tsv', 'x') as f:\n",
    "#     print(labels_header, file=f)\n",
    "#     for label in labels:\n",
    "#         print(label, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('res/features_by_ex_with_token_scores.tsv', 'w') as f:\n",
    "#     print(features_header, file=f)\n",
    "#     for feature in features:\n",
    "#         print(feature, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('res/labels_by_ex_with_token_scores.tsv', 'w') as f:\n",
    "#     print(labels_header, file=f)\n",
    "#     for label in labels:\n",
    "#         print(label, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By evaluation (n = 222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_features_df = pd.read_csv(\n",
    "    'res/features_by_eval_with_token_scores.tsv',\n",
    "    sep='\\t'\n",
    ")\n",
    "ev_labels_df = pd.read_csv(\n",
    "    'res/labels_by_eval_with_token_scores.tsv',\n",
    "    sep='\\t'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
