{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epr_identified = pd.read_csv('more_files/OBS_ReadingCategories_EPR_1.csv').iloc[1:63,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "all_categories_flattened = set([str(x).strip().lower() for y in epr_identified.values.tolist() for x in y if str(x) != 'nan'])\n",
    "# for line in all_categories_flattened:\n",
    "#     print([ps.stem(x) for x in line.split()])\n",
    "\n",
    "all_categories_stemmed = {' '.join([ps.stem(x) for x in line.split()]) for line in all_categories_flattened}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'data_first_email/20190118_reading_specialists.csv'\n",
    ")\n",
    "obs_full = list(df.OBS)\n",
    "obs_full = [sent.replace('word by word', 'word-by-word') for sent in obs_full]\n",
    "obs_short = [[y.strip() for y in x.lower().replace('$obs:', '').strip().split(';')] for x in obs_full]\n",
    "corpus = [x.replace('.', '').replace(',', '').replace('/', '') for y in obs_short for x in y if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "unidentified = []\n",
    "for line in corpus:\n",
    "    tokens = set(line.strip().split())\n",
    "    tokens = {ps.stem(x) for x in tokens}\n",
    "    match = False\n",
    "    for identified in all_categories_stemmed:\n",
    "        if len(tokens & set(identified.strip().split())) / len(tokens) > .5:\n",
    "            match = True\n",
    "            break\n",
    "    if not match:\n",
    "        unidentified.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_(req_words, corpus):\n",
    "    for req_words_set in req_words:\n",
    "        assert type(req_words_set) == type(set())\n",
    "    stemmed_req_words = [{ps.stem(wd) for wd in req_words_set} for req_words_set in req_words]\n",
    "    \n",
    "    caught = []\n",
    "    not_caught = []\n",
    "    for line in corpus:\n",
    "        stemmed = set([ps.stem(x) for x in line.strip().split()])\n",
    "        distinct = False\n",
    "        for stemmed_set in stemmed_req_words:\n",
    "            if stemmed_set & stemmed == set():\n",
    "                distinct = True\n",
    "        if not distinct:\n",
    "            caught.append(line)\n",
    "        else:\n",
    "            not_caught.append(line)\n",
    "    return caught, not_caught\n",
    "\n",
    "difficulty_text, leftover = filter_([\n",
    "    {'text', 'passage'},\n",
    "    {'easy', 'challenging', 'difficult', 'level', 'hard'}\n",
    "], unidentified)\n",
    "\n",
    "accuracy_text, leftover = filter_([\n",
    "    {'accurate', 'accuracy', 'miscue', 'misread'}\n",
    "], leftover)\n",
    "\n",
    "fluency_text, leftover = filter_([\n",
    "    {'fluency', 'fluent', 'fluently', 'disfluent', 'intonation', 'decode', 'word-by-word', 'choppy'}\n",
    "], leftover)\n",
    "\n",
    "pronunciation_text, leftover = filter_([\n",
    "    {'pronounce', 'pronunciation', 'enunciate', 'mispronounce', 'mispronunciation', 'attack'}\n",
    "], leftover)\n",
    "\n",
    "phrasing_text, leftover = filter_([\n",
    "    {'phrasing'}\n",
    "], leftover)\n",
    "\n",
    "meaning_text, leftover = filter_([\n",
    "    {'understanding', 'meaning', 'comprehension'}\n",
    "], leftover)\n",
    "\n",
    "speed_text, leftover = filter_([\n",
    "    {'fast', 'quick', 'slow', 'slowly', 'rush', 'rate', 'quickly', 'pace'}\n",
    "], leftover)\n",
    "\n",
    "subst_omit_text, leftover = filter_([\n",
    "    {'substitution', 'omission', 'omit', 'insert'}\n",
    "], leftover)\n",
    "\n",
    "punctuation_text, leftover = filter_([\n",
    "    {'punctuation'}\n",
    "], leftover)\n",
    "\n",
    "self_correct_text, leftover = filter_([\n",
    "    {'self-correct', 'correct', 'self'}\n",
    "], leftover)\n",
    "\n",
    "expression_text, leftover = filter_([\n",
    "    {'expression', 'monotone'}\n",
    "], leftover)\n",
    "\n",
    "volume_text, leftover = filter_([\n",
    "    {'quiet', 'volume'}\n",
    "], leftover)\n",
    "\n",
    "vocab_text, leftover = filter_([\n",
    "    {'vocabulary'}\n",
    "], leftover)\n",
    "\n",
    "qualitative_text, leftover = filter_([\n",
    "    {'good', 'solid', 'strong'}\n",
    "], leftover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "  - noticing first letter then guessing\n",
      "  - poor signal\n",
      "  - needs further instruction in explicit word attackmultisyllabic word skills\n",
      "  - $obs\n",
      "  - careful reader\n",
      "  - provide instruction in medial vowel patterns r-controlled vowel\n",
      "  - occasionally runs over periods\n",
      "  - consonant and vowel sounds were often incorrect\n",
      "  - second language learner\n",
      "  - lost place in text\n",
      "  - trouble with long vowel sounds (cement plywood recreation)\n",
      "  - hoarse voice quality\n",
      "  - elides unstressed syllable (can provide fish)\n",
      "  - difficulty with long vowel sounds and vowel teams\n",
      "  - observes some periods\n",
      "1\n",
      "  - difficulty with the prefix un- in unusual\n",
      "  - difficulty with the phoneme ph-\n",
      "  - difficulty articulating r sound\n",
      "  - some disfluency with stopping at periods\n",
      "  - difficulty with initial sounds\n",
      "  - has difficulty reading possessives\n",
      "2\n",
      "  - leaves off -er ending on angler\n",
      "  - student removing ending of words '-ed'\n",
      "  - student pausing in between sentences inconsistently\n",
      "  - [how did student even get this text?]\n",
      "  - drops endings (frame instead of framing)\n",
      "  - student repeating certain words at the end of a sentence\n",
      "  - student repeating words as they read\n",
      "  - student repeating words when first forgetting to include ending\n",
      "  - student \"fakes\" several sentences and skips to end\n",
      "3\n",
      "  - able to take apart multisyllabic words on the fly\n",
      "  - able to take apart multisyllabic words in text\n",
      "  - seems to be on right level\n",
      "  - able to take apart compound words (something slackline)\n",
      "  - difficulty taking apart multisyllabic words (specific accomplish photography)\n",
      "  - demonstrates ability to take apart words on the fly\n",
      "  - able to read past a word and then come back to it to solve it with context\n",
      "  - does not demonstrate flexibility with taking apart words\n",
      "4\n",
      "  - reads what he thinks would make sense in the sentence but not what is on the page\n",
      "  - reads a bit too rapidly at points which may (or may not) indicate that student is not comprehending\n",
      "  - reads in syllables not blended into words\n",
      "  - some emphasis on the wrong words within a sentence made the sentences sound disjointed\n",
      "  - left off the -y in photography\n",
      "  - student not pausing long enough in between sentences\n",
      "  - note: there is a period missing in the passage after \"on a card\"\n",
      "5\n",
      "  - successfully reads past tense verbs (-ed endings)\n",
      "  - some evidence of control of initial consonant blends\n",
      "  - evidence of past tense verb knowledge\n",
      "  - evidence of searching behaviors\n",
      "6\n",
      "  - skips over unknown words\n",
      "  - replaced certain sighthigh-frequency words with others (offof thea)\n",
      "  - familiar with some basic sight words\n",
      "  - has many beginning and ending sounds\n",
      "  - familiar with sigh words and many high frequency words\n",
      "  - knows a few high frequency words (of the)\n",
      "  - skips 'agates and jasper'\n",
      "  - skips 'activity' and instead says 'active'\n",
      "  - mumbles when encountering difficult words\n",
      "  - limited data as student did not read much out loud stopping when encountering tricky words and scanning ahead in the text for something familiar\n",
      "  - familiar with some beginning and ending sounds some sight words and high frequency words\n",
      "  - finds and reads a few familiar words without successfully articulating any sentences\n",
      "  - attempts few words\n",
      "7\n",
      "  - misses ea sound\n",
      "8\n",
      "  - reads quietly\n",
      "  - reads through most commas without pausing\n",
      "  - reading letter names instead of letter sounds\n",
      "  - reading unintelligible\n",
      "  - does not read whole passage aloud\n",
      "  - reads quite rapidly\n",
      "  - less than half text read\n",
      "9\n",
      "  - noticing names of people\n",
      "  - attends to the beginnings of words\n",
      "  - over-relying on sight words (of to the)\n",
      "  - doesn't always attend to word endings\n",
      "  - attending to beginning sounds but not the entire word when encountering a tricky word\n",
      "  - seems to potentially be using a picture to determine some words but unclear\n",
      "  - limited data--started late and stopped in the middle of the passage\n",
      "  - does not always cross-check multiple sources of information\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "def preprocess(line):\n",
    "    return ' '.join([ps.stem(x) for x in line.strip().split()])\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocess)\n",
    "tfidf = tfidf_vectorizer.fit_transform(leftover)\n",
    "kmeans = KMeans(n_clusters=10).fit(tfidf)\n",
    "line_to_cluster = {line: kmeans.predict(tfidf_vectorizer.transform([line]))[0] for line in leftover}\n",
    "for center in set(line_to_cluster.values()):\n",
    "    print(center)\n",
    "    lines_with_center = [x for x in line_to_cluster if line_to_cluster[x] == center]\n",
    "    for line in lines_with_center:\n",
    "        print('  -', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
